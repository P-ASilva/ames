{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path.cwd().parent / 'data'\n",
    "print(DATA_DIR)\n",
    "clean_data_path = DATA_DIR / 'processed' / 'ames_clean.pkl'\n",
    "with open(clean_data_path, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonNull_percent(label, null_value = 0 ):\n",
    "    f = data[label] != null_value\n",
    "    return (data[label][f].value_counts().sum()/data[label].shape[0] *100)\n",
    "\n",
    "def heatMapCorr(labels):\n",
    "    correlation_matrix = data[labels + [\"SalePrice\"]].corr()\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumm = []\n",
    "model_data = data.copy()\n",
    "categorical_columns = []\n",
    "ordinal_columns = []\n",
    "for col in model_data.select_dtypes('category').columns:\n",
    "    if model_data[col].cat.ordered:\n",
    "        ordinal_columns.append(col)\n",
    "    else:\n",
    "        categorical_columns.append(col)\n",
    "for col in ordinal_columns:\n",
    "    codes, _ = pd.factorize(data[col], sort=True)\n",
    "    model_data[col] = codes\n",
    "original_data = model_data['Exterior']\n",
    "encoded_data = pd.get_dummies(original_data)\n",
    "\n",
    "aux_dataframe = encoded_data\n",
    "aux_dataframe['Exterior'] = original_data.copy()\n",
    "\n",
    "aux_dataframe.head().transpose()\n",
    "original_data = model_data['Exterior']\n",
    "encoded_data = pd.get_dummies(original_data, drop_first=True)\n",
    "\n",
    "aux_dataframe = encoded_data\n",
    "aux_dataframe['Exterior'] = original_data.copy()\n",
    "\n",
    "aux_dataframe.head().transpose()\n",
    "model_data = pd.get_dummies(model_data, drop_first=True)\n",
    "model_data.info()\n",
    "for cat in categorical_columns:\n",
    "    dummies = []\n",
    "    for col in model_data.columns:\n",
    "        if col.startswith(cat + \"_\"):\n",
    "            dumm.append(col)\n",
    "            dummies.append(f'\"{col}\"')\n",
    "    dummies_str = ', '.join(dummies)\n",
    "    print(f'From column \"{cat}\" we made {dummies_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model_data\n",
    "data.hist(figsize=(35,35))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the distribution and format of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot related features\n",
    "lot_cat = [\"Lot.Frontage\",\"Lot.Area\",\"Lot.Shape\"]\n",
    "heatMapCorr(lot_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all seem relevant to the end result, so let's check their looks\n",
    "data[lot_cat].hist(bins=20)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like lot area might use some logs\n",
    "pd.DataFrame(np.log10(data[\"Lot.Area\"])).hist()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'correlation with target: {data[[\"Land.Slope\",\"SalePrice\"]].corr()[\"SalePrice\"][0]}')\n",
    "# not the highest correlation, so let's check how much of it isn't null or common:\n",
    "nonNull_percent(\"Land.Slope\")\n",
    "# only four percent of the data has a non-null value for this feature, so I will store it for now as a potential removal, as it also does not impact too much acording to the correlation.\n",
    "lessThan5p = [\"Land.Slope\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverAll features\n",
    "over_cat = [\"Overall.Qual\",\"Overall.Cond\"]\n",
    "data[over_cat].hist(bins=20) # these seem to reflect a rating\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Overall.Rat\"]  = data[\"Overall.Cond\"] + data[\"Overall.Qual\"]\n",
    "\n",
    "heatMapCorr(over_cat)\n",
    "# the overWhelming majority of houses in the dataSet are 4 in cond, so let's see if the remainder is significant\n",
    "print(nonNull_percent(\"Overall.Cond\",4))\n",
    "# it is, so the feature may still differ one house to another in quite a few cases, no alterations will be done here and store as ratings\n",
    "ratings = over_cat # we may like to avoid transforming those.\n",
    "\n",
    "# data = data.drop(columns=over_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[[\"Mas.Vnr.Area\",\"SalePrice\"]].corr()[\"SalePrice\"][0]) # high correlation...\n",
    "data[\"Mas.Vnr.Area\"].hist()\n",
    "print(nonNull_percent(\"Mas.Vnr.Area\")) # has a good non-null amount...\n",
    "right_skewed = [\"Mas.Vnr.Area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External features\n",
    "exter_cat = [\"Exter.Qual\",\"Exter.Cond\"]\n",
    "data[exter_cat].hist(bins=20) # these seem to reflect a more umbalanced rating\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Exter.Rat\"] = data[\"Exter.Qual\"] + data[\"Exter.Cond\"]\n",
    "heatMapCorr(exter_cat)\n",
    "# the overWhelming majority of houses in the dataSet are rated 2 on both, so let's see if the remainder is significant\n",
    "print(f\"qual: {nonNull_percent(exter_cat[0],2)}\")\n",
    "print(f\"cond: {nonNull_percent(exter_cat[1],2)}\")\n",
    "# it is, so the feature may still differ one house to another in quite a few cases, no alterations will be done here and be stored as ratings\n",
    "ratings.append(exter_cat[0])\n",
    "ratings.append(exter_cat[1])\n",
    "# data = data.drop(columns=exter_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basement Features (Area)\n",
    "basement_cat = [\"BsmtFin.SF.1\",\"BsmtFin.SF.2\", \"Bsmt.Unf.SF\",\"Total.Bsmt.SF\"]\n",
    "heatMapCorr(basement_cat) # checking relevance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[basement_cat].hist()\n",
    "for cat in basement_cat:\n",
    "    print(cat, nonNull_percent(cat))\n",
    "    \n",
    "right_skewed += [\"BsmtFin.SF.1\",\"BsmtFin.SF.2\", \"Bsmt.Unf.SF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heating and Eletrical categories\n",
    "he_cat = [\"Electrical\",\"Heating.QC\"]\n",
    "heatMapCorr(he_cat)  # seems relevant enough\n",
    "data[he_cat].hist()\n",
    "print(nonNull_percent(he_cat[0]))\n",
    "print(nonNull_percent(he_cat[1]))\n",
    "# storing the cat\n",
    "categories = he_cat\n",
    "right_skewed.append(he_cat[0])\n",
    "right_skewed.append(he_cat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xcat features\n",
    "xcat = [\"X1st.Flr.SF\",\"X2nd.Flr.SF\"] \n",
    "heatMapCorr(xcat)\n",
    "data[xcat].hist()\n",
    "print(nonNull_percent(xcat[0]))\n",
    "print(nonNull_percent(xcat[1]))\n",
    "# No worries it seems, still, what is it?\n",
    "right_skewed.append(xcat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"Low.Qual.Fin.SF\",\"SalePrice\"]].corr()[\"SalePrice\"][0]\n",
    "nonNull_percent(\"Low.Qual.Fin.SF\")\n",
    "lessThan5p.append(\"Low.Qual.Fin.SF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms = [\"Full.Bath\",\"Half.Bath\", \"Kitchen.AbvGr\", \"TotRms.AbvGrd\"]\n",
    "data[\"SqFtPerRoom\"] =  data[\"Gr.Liv.Area\"] / (data[\"TotRms.AbvGrd\"] +\n",
    "                                                       data[\"Full.Bath\"] +\n",
    "                                                       data[\"Half.Bath\"] +\n",
    "                                                       data[\"Kitchen.AbvGr\"])\n",
    "heatMapCorr(rooms+[\"SqFtPerRoom\",\"Gr.Liv.Area\"])\n",
    "data[rooms + [\"SqFtPerRoom\",\"Gr.Liv.Area\"]].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porch Features\n",
    "porch_cat = [\"Enclosed.Porch\", \"Screen.Porch\", \"X3Ssn.Porch\"]\n",
    "heatMapCorr(porch_cat) # low relevance for everyone\n",
    "\n",
    "\n",
    "for cat in porch_cat:\n",
    "    print(nonNull_percent(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We could drop those...\n",
    "dropList = []\n",
    "# pool area - pool area =D\n",
    "nonNull_percent(\"Pool.Area\")\n",
    "dropList.append(\"Pool.Area\") # questionable but...\n",
    "\n",
    "# Misc.Val - $Value of miscellaneous feature\n",
    "nonNull_percent(\"Misc.Val\")\n",
    "dropList.append(\"Misc.Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Initialize the quantile transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.log10(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer, RobustScaler, StandardScaler, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "model_data = data.copy()\n",
    "# model_data.drop(columns=lessThan5p)\n",
    "model_data = model_data.drop(columns=dropList)\n",
    "\n",
    "\n",
    "y = model_data[\"SalePrice\"].copy()\n",
    "X = model_data.drop(columns=[\"SalePrice\"]).copy()\n",
    "        \n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=0.25,random_state=12,)\n",
    "\n",
    "colT = ColumnTransformer(transformers=[\n",
    "    (\"log\", Pipeline([\n",
    "        (\"Log10\",LogTransformer())\n",
    "    ]), [\"Lot.Area\"]),\n",
    "        (\"Ppow\", Pipeline([\n",
    "        (\"pow\",PowerTransformer())\n",
    "    ]), right_skewed),\n",
    "], remainder=\"passthrough\")\n",
    "\n",
    "piped_model = Pipeline([\n",
    "    (\"column transformer\", colT),\n",
    "    # (\"Poly\", PolynomialFeatures()),\n",
    "    (\"Scaler\", StandardScaler()),\n",
    "    (\"Ridge\", Ridge())\n",
    "])\n",
    "\n",
    "\n",
    "param_grid_r = {\n",
    "    'Ridge__alpha': [0.1, 1.0, 10.0],  # List of alpha values to try\n",
    "    # 'Poly__degree': [0, 1, 2],\n",
    "}\n",
    "\n",
    "param_grid_cat = {\n",
    "    'iterations': [100, 200, 300],  # Number of boosting iterations\n",
    "    'depth': [4, 6, 8],             # Depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
    "}\n",
    "\n",
    "grid_search_r = GridSearchCV(piped_model, param_grid_r, \n",
    "                              cv=5)\n",
    "# grid_search_cat = GridSearchCV(CatBoostRegressor(), param_grid_cat, \n",
    "#                               cv=5)\n",
    "\n",
    "grid_search_r.fit(Xtrain,ytrain)\n",
    "\n",
    "ypred = grid_search_r.predict(Xtest)\n",
    "RMSE = np.sqrt(mean_squared_error(ytest, ypred))\n",
    "print(f\"RMSE = {RMSE}\")\n",
    "error_percent = 100 * (10**RMSE - 1)\n",
    "print(f'Average error is {error_percent:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
