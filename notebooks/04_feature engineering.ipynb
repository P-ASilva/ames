{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path.cwd().parent / 'data'\n",
    "print(DATA_DIR)\n",
    "clean_data_path = DATA_DIR / 'processed' / 'ames_clean.pkl'\n",
    "with open(clean_data_path, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonNull_percent(label, null_value = 0 ):\n",
    "    f = data[label] != null_value\n",
    "    return (data[label][f].value_counts().sum()/data[label].shape[0] *100)\n",
    "\n",
    "def heatMapCorr(labels):\n",
    "    correlation_matrix = data[labels + [\"SalePrice\"]].corr()\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumm = []\n",
    "model_data = data.copy()\n",
    "categorical_columns = []\n",
    "ordinal_columns = []\n",
    "for col in model_data.select_dtypes('category').columns:\n",
    "    if model_data[col].cat.ordered:\n",
    "        ordinal_columns.append(col)\n",
    "    else:\n",
    "        categorical_columns.append(col)\n",
    "for col in ordinal_columns:\n",
    "    codes, _ = pd.factorize(data[col], sort=True)\n",
    "    model_data[col] = codes\n",
    "original_data = model_data['Exterior']\n",
    "encoded_data = pd.get_dummies(original_data)\n",
    "\n",
    "aux_dataframe = encoded_data\n",
    "aux_dataframe['Exterior'] = original_data.copy()\n",
    "\n",
    "aux_dataframe.head().transpose()\n",
    "original_data = model_data['Exterior']\n",
    "encoded_data = pd.get_dummies(original_data, drop_first=True)\n",
    "\n",
    "aux_dataframe = encoded_data\n",
    "aux_dataframe['Exterior'] = original_data.copy()\n",
    "\n",
    "aux_dataframe.head().transpose()\n",
    "model_data = pd.get_dummies(model_data, drop_first=True)\n",
    "model_data.info()\n",
    "for cat in categorical_columns:\n",
    "    dummies = []\n",
    "    for col in model_data.columns:\n",
    "        if col.startswith(cat + \"_\"):\n",
    "            dumm.append(col)\n",
    "            dummies.append(f'\"{col}\"')\n",
    "    dummies_str = ', '.join(dummies)\n",
    "    print(f'From column \"{cat}\" we made {dummies_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model_data\n",
    "data.hist(figsize=(35,35))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the distribution and format of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot related features\n",
    "lot_cat = [\"Lot.Frontage\",\"Lot.Area\",\"Lot.Shape\"]\n",
    "heatMapCorr(lot_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all seem relevant to the end result, so let's check their looks\n",
    "data[lot_cat].hist(bins=20)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like lot area might use some logs\n",
    "pd.DataFrame(np.log10(data[\"Lot.Area\"])).hist()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'correlation with target: {data[[\"Land.Slope\",\"SalePrice\"]].corr()[\"SalePrice\"][0]}')\n",
    "# not the highest correlation, so let's check how much of it isn't null or common:\n",
    "nonNull_percent(\"Land.Slope\")\n",
    "# only four percent of the data has a non-null value for this feature, so I will store it for now as a potential removal, as it also does not impact too much acording to the correlation.\n",
    "lessThan5p = [\"Land.Slope\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverAll features\n",
    "over_cat = [\"Overall.Qual\",\"Overall.Cond\"]\n",
    "data[over_cat].hist(bins=20) # these seem to reflect a rating\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Overall.Rat\"]  = data[\"Overall.Cond\"] + data[\"Overall.Qual\"]\n",
    "\n",
    "heatMapCorr(over_cat)\n",
    "# the overWhelming majority of houses in the dataSet are 4 in cond, so let's see if the remainder is significant\n",
    "print(nonNull_percent(\"Overall.Cond\",4))\n",
    "# it is, so the feature may still differ one house to another in quite a few cases, no alterations will be done here and store as ratings\n",
    "ratings = over_cat # we may like to avoid transforming those.\n",
    "\n",
    "# data = data.drop(columns=over_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[[\"Mas.Vnr.Area\",\"SalePrice\"]].corr()[\"SalePrice\"][0]) # high correlation...\n",
    "data[\"Mas.Vnr.Area\"].hist()\n",
    "print(nonNull_percent(\"Mas.Vnr.Area\")) # has a good non-null amount...\n",
    "right_skewed = [\"Mas.Vnr.Area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External features\n",
    "exter_cat = [\"Exter.Qual\",\"Exter.Cond\"]\n",
    "data[exter_cat].hist(bins=20) # these seem to reflect a more umbalanced rating\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Exter.Rat\"] = data[\"Exter.Qual\"] + data[\"Exter.Cond\"]\n",
    "heatMapCorr(exter_cat)\n",
    "# the overWhelming majority of houses in the dataSet are rated 2 on both, so let's see if the remainder is significant\n",
    "print(f\"qual: {nonNull_percent(exter_cat[0],2)}\")\n",
    "print(f\"cond: {nonNull_percent(exter_cat[1],2)}\")\n",
    "# it is, so the feature may still differ one house to another in quite a few cases, no alterations will be done here and be stored as ratings\n",
    "ratings.append(exter_cat[0])\n",
    "ratings.append(exter_cat[1])\n",
    "# data = data.drop(columns=exter_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basement Features (Area)\n",
    "basement_cat = [\"BsmtFin.SF.1\",\"BsmtFin.SF.2\", \"Bsmt.Unf.SF\",\"Total.Bsmt.SF\"]\n",
    "heatMapCorr(basement_cat) # checking relevance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[basement_cat].hist()\n",
    "for cat in basement_cat:\n",
    "    print(cat, nonNull_percent(cat))\n",
    "    \n",
    "right_skewed += [\"BsmtFin.SF.1\",\"BsmtFin.SF.2\", \"Bsmt.Unf.SF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heating and Eletrical categories\n",
    "he_cat = [\"Electrical\",\"Heating.QC\"]\n",
    "heatMapCorr(he_cat)  # seems relevant enough\n",
    "data[he_cat].hist()\n",
    "print(nonNull_percent(he_cat[0]))\n",
    "print(nonNull_percent(he_cat[1]))\n",
    "# storing the cat\n",
    "categories = he_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xcat features\n",
    "xcat = [\"X1st.Flr.SF\",\"X2nd.Flr.SF\"] \n",
    "heatMapCorr(xcat)\n",
    "data[xcat].hist()\n",
    "print(nonNull_percent(xcat[0]))\n",
    "print(nonNull_percent(xcat[1]))\n",
    "# No worries it seems, still, what is it?\n",
    "right_skewed.append(xcat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"Low.Qual.Fin.SF\",\"SalePrice\"]].corr()[\"SalePrice\"][0]\n",
    "nonNull_percent(\"Low.Qual.Fin.SF\")\n",
    "lessThan5p.append(\"Low.Qual.Fin.SF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms = [\"Full.Bath\",\"Half.Bath\", \"Kitchen.AbvGr\", \"TotRms.AbvGrd\"]\n",
    "data[\"SqFtPerRoom\"] =  data[\"Gr.Liv.Area\"] / (data[\"TotRms.AbvGrd\"] +\n",
    "                                                       data[\"Full.Bath\"] +\n",
    "                                                       data[\"Half.Bath\"] +\n",
    "                                                       data[\"Kitchen.AbvGr\"])\n",
    "heatMapCorr(rooms+[\"SqFtPerRoom\",\"Gr.Liv.Area\"])\n",
    "data[rooms + [\"SqFtPerRoom\",\"Gr.Liv.Area\"]].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porch Features\n",
    "porch_cat = [\"Enclosed.Porch\", \"Screen.Porch\", \"X3Ssn.Porch\"]\n",
    "heatMapCorr(porch_cat) # low relevance for everyone\n",
    "\n",
    "\n",
    "for cat in porch_cat:\n",
    "    print(nonNull_percent(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[\"Pool.Area\",\"Misc.Val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Lot.Area\"] = np.log10(data[\"Lot.Area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "data[right_skewed].hist()\n",
    "p = PowerTransformer()\n",
    "data[right_skewed] = p.fit_transform(data[right_skewed])\n",
    "data[right_skewed].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_path = DATA_DIR / 'processed' / 'ames_clean_eng.pkl'\n",
    "with open(clean_data_path, 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
